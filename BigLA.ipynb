{
 "cells": [
  {
   "cell_type": "markdown",f
   "metadata": {},
   "source": [
    "# Distributed Linear Algebra on Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### - Brief Apache Spark Intro\n",
    "\n",
    "## Part 2\n",
    "### - Linear regression problem formulation\n",
    "### - Complexity of the closed form solution at scale\n",
    "### - Big n Small d\n",
    "### - Big n Big d\n",
    "### - Iterative approach for Big n Big d\n",
    "### - Parallel gradient descent\n",
    "\n",
    "## Part 3 \n",
    "### - Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark is a fast and general engine for large-scale data processing\n",
    "![Spark Libs](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/spark-libs.png)\n",
    "\n",
    "### Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\n",
    "![Spark Compatabilities](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/spark-cmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hadoop data sharing](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/data-sharing-mapreduce.png)\n",
    "![Spark data sharing](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/data-sharing-spark.png)\n",
    "Spark and Shark: Lightning-Fast Analytics over Hadoop and Hive Data:  http://www.slideshare.net/jetlore/spark-and-shark-lightningfast-analytics-over-hadoop-and-hive-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Model:\n",
    "\n",
    "### Spark provides two main abstractions for parallel programming: \n",
    "### - resilient distributed datasets (RDD) \n",
    "### - parallel operations on these datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Map a function on different nodes localy\n",
    "squaredRDD = rdd.map(lambda i: i*i)\n",
    "\n",
    "# Send data to one node for reduce operation\n",
    "squaredRDD.reduce(lambda a,b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression problem formulation\n",
    "###$n$ - training observation, $d$ - number of features\n",
    "\n",
    "###$\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$: dataset matrix \n",
    "\n",
    "###$\\mathbf{y} \\in \\mathbb{R}^{n}$: labels\n",
    "\n",
    "###$\\mathbf{\\hat{y}} \\in \\mathbb{R}^{n}$: predictions, where $\\hat{y} = Xw$\n",
    "\n",
    "###$\\mathbf{w} \\in \\mathbb{R}^{d}$: regression parameters we want to learn\n",
    "\n",
    "###$\\mathbf{\\min_w ||Xw - y||^{2}_{2}} $: objective function to learn the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution \n",
    "\n",
    "###$\\mathit{\\frac{df}{dw}(w)} = \\mathbf{wX^{T}X - X^{T}y} = 0$\n",
    "\n",
    "###$\\mathbf{w} = \\mathbf{(X^{T}X)^{-1}X^{T}y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU bottlenecks:\n",
    "### Matrix multiplication $\\mathbf{X^{T}X}$: $O(nd^2)$ operations\n",
    "\n",
    "### Matrix inverse $^{-1}$: $O(d^{3})$ operations\n",
    "\n",
    "### Space bottlenecks:\n",
    "### $\\mathbf{X}$: $O(nd)$ float64\n",
    "### $\\mathbf{X^{T}X}$ and inverse $^{-1}$: $O(d^{2})$ float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if X is overdetermined? Big n Small d\n",
    "\n",
    "### So that $O(nd^{2})$ to long to compute on single node\n",
    "### $O(nd)$ to big to store on single node\n",
    "\n",
    "### But $O(d^{3})$ and $O(d^{2})$ is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: find the way to distribute space and CPU utilization\n",
    "### - Split observations (rows of the matrix $\\mathbf{X}$) between many nodes\n",
    "### - Use outer product instead of inner product for matrix multiplication\n",
    "![Matrix multiplication using inner products](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/matmul.png)\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Matrix_multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###$\\begin{vmatrix} 1 & -3 & 2 \\\\ 4 & 5 & -2 \\end{vmatrix} \\begin{vmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 5 & 6 \\end{vmatrix}  = \\begin{vmatrix} 1-6+10 & 3-12+12 \\\\ 4+10-10 & 12+20-12 \\end{vmatrix} = \\begin{vmatrix} 5 & 3 \\\\ 4 & 20 \\end{vmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of outer products\n",
    "### $\\begin{vmatrix} 1 & -3 & 2 \\\\ 4 & 5 & -2 \\end{vmatrix} \\begin{vmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 5 & 6 \\end{vmatrix} = \\begin{vmatrix} 1 & 3 \\\\ 4 & 12 \\end{vmatrix} + \\begin{vmatrix} -6 & -12 \\\\ 10 & 20 \\end{vmatrix} + \\begin{vmatrix} 10 & 12 \\\\ -10 & -12 \\end{vmatrix} = \\begin{vmatrix} 5 & 3 \\\\ 4 & 20 \\end{vmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement $\\mathbf{(X^{T}X)^{-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00816462, -0.00341924, -0.00336384],\n",
       "       [-0.00341924,  0.00818102, -0.00354132],\n",
       "       [-0.00336384, -0.00354132,  0.00803881]])"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "n = 1000\n",
    "d = 3\n",
    "x = numpy.random.random((n, d))\n",
    "rdd = sc.parallelize(x) # Create RDD\n",
    "\n",
    "# Compute outer product for each row\n",
    "outerProductsRDD = rdd.map(lambda row: numpy.outer(row, row))\n",
    "\n",
    "# Reduce, sum up and compute inverse\n",
    "matrixSum = outerProductsRDD.reduce(lambda a,b: a + b)\n",
    "numpy.linalg.inv(matrixSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big n Big d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{w} = \\mathbf{(X^{T}X)^{-1}X^{T}y}$: now all operations and storage are bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form solution can not be computed here\n",
    "\n",
    "### But we can try to find approximate solution iteratively\n",
    "\n",
    "![Gradient Descent](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/gd.png)\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent update rule\n",
    "### $\\mathbf{w}_{i+1} = \\mathbf{w}_i + \\alpha \\mathit{\\frac{df}{dw}(\\mathbf{w}_i)}$\n",
    "### $\\mathbf{w}_{i+1} = \\mathbf{w}_i + \\alpha \\sum_{j=1}^{n}{(\\mathbf{w}_i^T \\mathbf{x}^{(j)} - y^{(j)}) \\mathbf{x}^{(j)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that: \n",
    "### - $(\\mathbf{w}_i^T \\mathbf{x}^{(j)} - y^{(j)}) \\mathbf{x}^{(j)}$ is just $O(d)$ in CPU and Space complexity\n",
    "### - Sum is commutative and associative operation\n",
    "\n",
    "### Solution: compute gradient in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166665904917\n",
      "0.245233372993\n",
      "0.294263227894\n",
      "0.32855280764\n",
      "0.354110499495\n",
      "0.373963356251\n",
      "0.389842263108\n",
      "0.402823885251\n",
      "0.413621045657\n",
      "0.422726749084\n",
      "0.430492696973\n",
      "0.437180752912\n",
      "0.442988664943\n",
      "0.448067332219\n",
      "0.452536552263\n",
      "0.456491528197\n",
      "0.460009065903\n",
      "0.463151054441\n",
      "0.465968861532\n",
      "0.468505467739\n",
      "0.470796628863\n",
      "0.472872018713\n",
      "0.474757757116\n",
      "0.476475357058\n",
      "0.478043810283\n",
      "0.479479237352\n",
      "0.480795686271\n",
      "0.482005616391\n",
      "0.483119576826\n",
      "0.48414670271\n",
      "0.48509597663\n",
      "0.485974458083\n",
      "0.486788394482\n",
      "0.487543829373\n",
      "0.488245394338\n",
      "0.488898478007\n",
      "0.489506754282\n",
      "0.490074011514\n",
      "0.490603870042\n",
      "0.491098948711\n",
      "0.491562508027\n",
      "0.491996181532\n",
      "0.492403120326\n",
      "0.492784729335\n",
      "0.493143145862\n",
      "0.493480324053\n",
      "0.493797323764\n",
      "0.494095600867\n",
      "0.494376609539\n",
      "0.494641619162\n"
     ]
    }
   ],
   "source": [
    "# Generate some data for our toy example\n",
    "n = 1000\n",
    "d = 1000\n",
    "trueW = numpy.tile(0.5, d)\n",
    "x = numpy.arange(0, 1, 1/float(n*d)).reshape(n,d)\n",
    "f = lambda x: x.dot(trueW) + numpy.random.normal(size=(d))\n",
    "rdd = sc.parallelize(x).map(lambda x: (x, f(x))) # Create RDD\n",
    "\n",
    "# Gradient Descent\n",
    "maxiter = 50       # Stop when reached\n",
    "alpha = 1.0e-3     # Initital step \n",
    "w = numpy.zeros(d) # start with all weights equal zero\n",
    "\n",
    "# Compute gradient element\n",
    "def grad(w, data):\n",
    "    x, y = data\n",
    "    return (w.dot(x) - y) * x\n",
    "\n",
    "for i in range(maxiter):\n",
    "    # Reduce step size by number of iterations\n",
    "    alph = alpha / (n * numpy.sqrt(i+1))\n",
    "    \n",
    "    # Compute gradient \n",
    "    gradient = (rdd.map(lambda row: grad(w, row))\n",
    "                   .sum()) \n",
    "    \n",
    "    # Update weights\n",
    "    w -= alph * gradient\n",
    "    print(w.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, but $w$ is out of scope! It's $O(d)$ in network communication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do more work localy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59561357911\n",
      "0.526428955047\n",
      "0.510887522063\n",
      "0.504822025432\n",
      "0.502100084647\n"
     ]
    }
   ],
   "source": [
    "fewIters = 5        # Do less iterations\n",
    "miniBatchSize = 32  # Use 20 vectors on each iteration\n",
    "alpha = 1.0e-4      # Initital step\n",
    "w = numpy.zeros(d)  # start with all weights equal zero\n",
    "frac = rdd.getNumPartitions() * miniBatchSize / float(n)\n",
    "\n",
    "# Compute stochastic gradient using mini batches\n",
    "for i in range(fewIters):\n",
    "    # Reduce step size on each iteration\n",
    "    alph = alpha / (n * numpy.sqrt(i+1))\n",
    "    \n",
    "    # Randomly sample rows from our dataset\n",
    "    sampleRDD = rdd.sample(True, frac)\n",
    "    \n",
    "    # Calculate updates locally\n",
    "    localUpdate = (sampleRDD.map(lambda row: grad(w, row))\n",
    "                            .mapPartitions(lambda itr: reduce(numpy.add, itr))\n",
    "                            .map(lambda gradient: alph * gradient)\n",
    "                            .sum())\n",
    "    \n",
    "    # Average update\n",
    "    meanUpdate = localUpdate / sampleRDD.getNumPartitions()\n",
    "    w -= meanUpdate\n",
    "    print(w.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Principal Component Analysis](https://raw.githubusercontent.com/gennadiikurabko/BigLA/master/files/pca.png)\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of the PCA \n",
    "### $\\mathbf{C} = \\mathbf{U \\Lambda U^T}$, where \n",
    "### $\\mathbf{C} = \\frac{1}{n} \\mathbf{X^TX}$: $d \\times d$ Covarience Matrix, features have zero mean\n",
    "### $\\mathbf{\\Lambda}$:  $d \\times d$ diagonal matrix with eigenvalues\n",
    "### $\\mathbf{U}$: $d \\times d$ matrix with eigenvectors in columns\n",
    "### - all eigenvectors point to the directon of max varience\n",
    "### - eigenvalues equal variance in these directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step algorithm\n",
    "### 1. Subtract mean\n",
    "### 2. Compute Covarience Matrix\n",
    "### 3. Eigendecomposition\n",
    "### 4. Choose $k$ top components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big n Small d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU bottlenecks:\n",
    "### $\\mathbf{X^{T}X}$: $O(nd^2)$ operations\n",
    "\n",
    "### Eigendecomposition: $O(d^{3})$ operations\n",
    "\n",
    "### Space bottlenecks:\n",
    "### $\\mathbf{X}$: $O(nd)$ float64\n",
    "### $\\mathbf{X^{T}X}$: $O(d^{2})$ float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: distributed PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of varience explained: \n",
      " [  9.99988485e-01   5.93379227e-06   5.58086173e-06]\n",
      "\n",
      "Top Component:\n",
      " [  7.07083178e-01  -7.07130383e-01  -2.88075133e-05]\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "n = 1000\n",
    "d = 3\n",
    "x = numpy.random.normal(size=(n, d))\n",
    "x[numpy.arange(1000),0] += numpy.arange(1000)\n",
    "x[numpy.arange(1000),1] -= numpy.arange(1000)\n",
    "rdd = sc.parallelize(x) # Create RDD\n",
    "\n",
    "# 1. Subtract mean\n",
    "m = rdd.mean()\n",
    "centeredRDD = rdd.map(lambda row: row - m)\n",
    "\n",
    "# 2. Compute Covariance Matrix\n",
    "covMatrix = (centeredRDD.map(lambda row: numpy.outer(row, row))\n",
    "                        .sum()) / n\n",
    "\n",
    "# 3. Eigendecomposition\n",
    "eigVal, eigVec = numpy.linalg.eigh(covMatrix)\n",
    "\n",
    "# 4. Choose k components\n",
    "idx = numpy.argsort(eigVal)\n",
    "varienceExplained = eigVal / sum(eigVal)\n",
    "topCmp = eigVec[:, idx[-1]]\n",
    "\n",
    "print 'Percent of varience explained: \\n {0}'.format(varienceExplained[::-1])\n",
    "print '\\nTop Component:\\n {0}'.format(topCmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced representation of original Matrix $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79383563804312995,\n",
       " 2.8573846561930027,\n",
       " 2.9789141930688401,\n",
       " 3.976044012939091,\n",
       " 5.7503788545863435,\n",
       " 7.2462330324482593,\n",
       " 8.5628624066254062,\n",
       " 9.8078607253685064,\n",
       " 11.461945123671729,\n",
       " 12.282522089092499]"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedMatrix = rdd.map(lambda row: row.dot(topCmp))\n",
    "reducedMatrix.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big n Big d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krylov subspace methods allows to compute PCA solution iteratively\n",
    "https://en.wikipedia.org/wiki/Arnoldi_iteration\n",
    "### Random projection methods \n",
    "https://en.wikipedia.org/wiki/Random_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Spark Documentation:\n",
    "http://spark.apache.org/docs/latest/\n",
    "\n",
    "### Spark and Matrix Factorization: \n",
    "http://stanford.edu/~rezab/slides/reza_codeneuro.pdf\n",
    "\n",
    "### Scalable Machine Learning\n",
    "https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x\n",
    "\n",
    "### ARPACK \n",
    "http://www.caam.rice.edu/software/ARPACK/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Linear Algebra on Apache Spark\n",
    "https://github.com/nikolaypavlov/BigLA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
